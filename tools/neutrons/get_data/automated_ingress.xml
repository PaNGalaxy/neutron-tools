<tool id="neutrons_ingress" name="Live Neutron Data Monitor" profile="22.05" version="0.4.0">
    <creator>
        <person name="NDIP Team"/>
        <organization url="https://www.ornl.gov/" name="ORNL"/>
    </creator>
    <requirements>
        <container shell="/bin/bash" type="docker">savannah.ornl.gov/ndip/tool-sources/generic/ingress:0.6.0
        </container>
    </requirements>
    <environment_variables>
        <environment_variable name="API_KEY" inject="api_key"/>
    </environment_variables>
    <command><![CDATA[
            #set $filter = $data_type.filter if $data_type.mode == "reduced" else "**"
            python3 -u /app/main.py
              --live-monitoring $live_monitor
              --live-monitoring-type kafka
              --ingress-previous-data $previous_data
              --facility $facility
              --instrument $instrument
              --ipts $ipts
              --data-type $data_type.mode
              --file-from-disk-filter "$filter"
              --kafka-url 10.64.193.19:9092
              --galaxy-url $__galaxy_url__
              --galaxy-user-api-key \$API_KEY
              --galaxy-history-name "$history"
              --galaxy-user-id $__user_id__
            #if $workflow.execute == "true"
              --execute-workflow $workflow.name
              --workflow-input-label "$workflow.ingress_label"
              #for $i in $workflow.extra_inputs
                --workflow-extra-input "$i.name":"$__app__.security.encode_id(i['file'].id)"
              #end for
            #end ifs
            > >(tee -a $output) 2> >(tee -a $output >&2)
    ]]></command>
    <inputs>
        <param name="facility" type="select" label="Facility Name" optional="false">
            <option value="SNS" selected="false">SNS</option>
            <option value="HFIR">HFIR</option>
        </param>
        <param name="instrument" type="select" label="Instrument Name" optional="false">
            <options from_data_table="instruments">
                <column name="name" index="3"/>
                <column name="value" index="2"/>
                <filter type="param_value" ref="facility" column="0"/>
            </options>
        </param>
        <param name="ipts" type="text" label="IPTS" value="IPTS-" optional="false"/>
        <param name="live_monitor" type="boolean" checked="true" label="Live monitor and ingress new files"/>
        <param name="previous_data" type="boolean" checked="false" label="Ingress old files"/>
        <conditional name="data_type">
            <param name="mode" type="select" label="Data Type" value="raw" optional="false">
                <option value="raw">Raw Data</option>
                <option value="reduced">Reduced Data</option>
            </param>
            <when value="reduced">
                <param name="filter" type="text" label="Filter Files" value="" optional="true" help="Wildcard to filter reduced data.
                E.g. *.dat (all files in autoreduce folder, no recursion); subfolder/*.dat (.dat files in subfolder);
                **/*.dat - all .dat files in autoreduce and subfolders;
                empty string (or **) - all files in autoreduce and all subfolders.
                "/>
            </when>
            >
        </conditional>
        <conditional name="workflow">
            <param name="execute" type="boolean" checked="false" label="Execute workflow after ingress"/>

            <when value="true">
                <param name="name" type="text" label="Workflow Name" value="" optional="false"/>
                <param name="ingress_label" type="text" label="Input parameter name for ingress" value=""
                       optional="false"/>
                <repeat name="extra_inputs" title="Workflow Extra Inputs">
                    <param name="name" type="text" label="Input parameter name" optional="false"/>
                    <param name="file" type="data" label="Data file"/>
                </repeat>
            </when>
            >
        </conditional>
        <param name="history" type="text" label="History Name" value="" optional="true"
               help="empty value will create history with IPTS number like IPTS-XXX"/>
    </inputs>
    <outputs>
        <data format="txt" name="output"
              label="Ingress $ipts data ($data_type.mode) to history ${str($history) if str($history)!='' else str($ipts)}">
        </data>
    </outputs>
    <help>
This tool enables users to automatically ingress their experiment data in realtime. 

----------
Both SNS and HFIR data are available. Specify which facility using the "Facility Name" parameter.
The "Instrument Name" parameter allows you to choose which instrument that you want to see data from.
The "IPTS" field allows you to enter their IPTS number to specify which experiment to see data from.

If you want to have multiple instruments, runs, or has experiments at both facilities, then you should
simply lauch an instance of this tool for each individual run, with the appropriate parameters supplied.

The "Live monitor and ingress new files" checkbox is the option that enables live data ingressing into Galaxy.
You can also ingress old files from a finished experiment using the "Ingress old files" checkbox. If both of these 
checkboxes are turned off (value of "No"), then the tool will fail. 

The "Data Type" field lets you choose whether you want Raw Data, or you can filter out the raw data and only see reduced data.
You can also choose a wildcard to filter your reduced data. For example, if you only ".dat" file types in the top level experiment directory,
you can set the Data Type to "Reduced Data", and then enter "*.dat" into the Filter Files field.

Some more examples of filtering include:

- *.dat (all files in autoreduce folder, no recursion);

- subfolder/*.dat (.dat files in subfolder)

- **/*.dat - all .dat files in autoreduce and subfolders

- empty string (or **) - all files in autoreduce and all subfolders



After you ingress data into Galaxy, you can automatically kick off a Galaxy workflow. First, you set "Execute workflow after ingress" to "Yes". From here,
supply the name of the workflow and importantly which workflow input will correspond to the data that you are ingressing. For example, if your workflow has two inputs, say
an input called "analysis_config" and then another called "reduced_data", and the "reduced_data" input is the parameter that you want to feed your live data into, then make sure that
you enter "reduced_data" into the "Input parameter name for ingress" field. You can also add extra inputs into the workflow from your existing Galaxy history. Click the Plus button in the section, and you can enter that input's name as well, alongside choosing which file *from your history* that you want to feed into that input. Using our previous example, if we had a config file in our history (called "analysis.config"), then we would enter "analysis.config" into the Input parameter name field, and then choose that file in the file picker. 

Finally, you can specify which history you want the data to ingress into. An empty value will create a new history, with the name being based off of the IPTS number for easy access. 
    </help>
</tool>
