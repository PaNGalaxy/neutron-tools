<tool id="ml-test-chat-prototype" name="MLFlow Chat" version="0.1.1" profile="22.01">
    <description>Train a model</description>
    <requirements>
        <container type="docker" shell="/bin/bash">savannah.ornl.gov/ndip/tool-sources/generic/ml-test/prototypes:f147cfa1edab5f8898b0f16fd7f8d2da75cc9781</container>
    </requirements>
     <command><![CDATA[
        #set $gemini_api_key = $__user__.preferences.get( "gemini_api_key", "None" )
        mkdir model &&
        export MLFLOW_LOGGING_LEVEL=ERROR &&
        pixi run --as-is --manifest-path /src python -m ml_poc
                                --url $url
                                --experiment $experiment
                            chat --prompt $request
                                 --gemini-api-key $gemini_api_key
                               > >(tee -a $output) 2> >(tee -a $output >&2)
    ]]></command>
    <inputs>
        <param name="url" type="text" value="http://10.64.193.19:5000" optional="false"  label="MLFlow tracking server url"/>
        <param name="experiment" type="text" value="gemini_chat" optional="false"  label="MLFlow experiment name"/>
        <param name="request" type="text" value="" label="Ask something"/>
    </inputs>
    <outputs>
        <data format="txt" name="output" label="Response"></data>
    </outputs>
    <help><![CDATA[
        **ML POC**

        This is a Galaxy tool that generates a Gemini response to your question.

        **Note** Add your Gemini API key (Click on your name in top menu -> Preferences -> Manage Ingormation)

        For more information, see the repository: https://code.ornl.gov/ndip/tool-sources/generic/ml-test
    ]]></help>
</tool>
